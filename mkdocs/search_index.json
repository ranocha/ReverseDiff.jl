{
    "docs": [
        {
            "location": "/", 
            "text": "ReverseDiff.jl\n\n\nThis documentation provides a specification of ReverseDiff's API, tips for usage, and details about the package's implementation.\n\n\nFor a basic introduction to ReverseDiff, see \nthe package's README\n.\n\n\nFor usage examples, see \nthe examples directory\n.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#reversediffjl", 
            "text": "This documentation provides a specification of ReverseDiff's API, tips for usage, and details about the package's implementation.  For a basic introduction to ReverseDiff, see  the package's README .  For usage examples, see  the examples directory .", 
            "title": "ReverseDiff.jl"
        }, 
        {
            "location": "/limits/", 
            "text": "Limitations of ReverseDiff\n\n\nReverseDiff works by injecting user code with new number types that record all operations that occur on them, accumulating an execution trace of the target function which can be re-run forwards and backwards to propagate new input values and derivative information. Naturally, this technique has some limitations. Here's a list of all the roadblocks we've seen users run into (\"target function\" here refers to the function being differentiated):\n\n\n\n\nThe target function can only be composed of generic Julia functions.\n ReverseDiff cannot propagate derivative information through non-Julia code. Thus, your function may not work if it makes calls to external, non-Julia programs, e.g. uses explicit BLAS calls instead of \nAx_mul_Bx\n-style functions.\n\n\nThe target function must be written generically enough to accept numbers of type \nT\n:Real\n as input (or arrays of these numbers).\n The function doesn't require a specific type signature, as long as the type signature is generic enough to avoid breaking this rule. This also means that any storage assigned used within the function must be generic as well.\n\n\nNested differentiation of closures is dangerous.\n Differentiating closures is safe, and nested differentation is safe, but you might be vulnerable to a subtle bug if you try to do both. See \nthis ForwardDiff issue\n for details. A fix is currently being planned for this problem.\n\n\nArray input types must obey \nA\n:AbstractArray\n and \nBase.IndexStyle(::A) == Base.IndexLinear()\n.\n\n\nArray inputs that are being differentiated cannot be mutated\n. This also applies to any \"descendent\" arrays that must be tracked (e.g. if \nA\n is an immutable input array, then \nC = A * A\n will also be immutable). If you try to perform \nsetindex!\n on such arrays, an error will be thrown. In the future, this restriction might be lifted. Note that arrays explicitly constructed within the target function (e.g. via \nones\n, \nsimilar\n, etc.) can be mutated, as well as output arrays used when taking the Jacobian of a function of the form \nf!(output, input....).", 
            "title": "Limitations of ReverseDiff"
        }, 
        {
            "location": "/limits/#limitations-of-reversediff", 
            "text": "ReverseDiff works by injecting user code with new number types that record all operations that occur on them, accumulating an execution trace of the target function which can be re-run forwards and backwards to propagate new input values and derivative information. Naturally, this technique has some limitations. Here's a list of all the roadblocks we've seen users run into (\"target function\" here refers to the function being differentiated):   The target function can only be composed of generic Julia functions.  ReverseDiff cannot propagate derivative information through non-Julia code. Thus, your function may not work if it makes calls to external, non-Julia programs, e.g. uses explicit BLAS calls instead of  Ax_mul_Bx -style functions.  The target function must be written generically enough to accept numbers of type  T :Real  as input (or arrays of these numbers).  The function doesn't require a specific type signature, as long as the type signature is generic enough to avoid breaking this rule. This also means that any storage assigned used within the function must be generic as well.  Nested differentiation of closures is dangerous.  Differentiating closures is safe, and nested differentation is safe, but you might be vulnerable to a subtle bug if you try to do both. See  this ForwardDiff issue  for details. A fix is currently being planned for this problem.  Array input types must obey  A :AbstractArray  and  Base.IndexStyle(::A) == Base.IndexLinear() .  Array inputs that are being differentiated cannot be mutated . This also applies to any \"descendent\" arrays that must be tracked (e.g. if  A  is an immutable input array, then  C = A * A  will also be immutable). If you try to perform  setindex!  on such arrays, an error will be thrown. In the future, this restriction might be lifted. Note that arrays explicitly constructed within the target function (e.g. via  ones ,  similar , etc.) can be mutated, as well as output arrays used when taking the Jacobian of a function of the form  f!(output, input....).", 
            "title": "Limitations of ReverseDiff"
        }, 
        {
            "location": "/api/", 
            "text": "ReverseDiff API\n\n\n\n\nGradients of \nf(x::AbstractArray{\n:Real}...)::Real\n\n\n#\n\n\nReverseDiff.gradient\n \n \nFunction\n.\n\n\nReverseDiff.gradient(f, input, cfg::GradientConfig = GradientConfig(input))\n\n\n\n\nIf \ninput\n is an \nAbstractArray\n, assume \nf\n has the form \nf(::AbstractArray{\n:Real})::Real\n and return \n\u2207f(input)\n.\n\n\nIf \ninput\n is a tuple of \nAbstractArray\ns, assume \nf\n has the form \nf(::AbstractArray{\n:Real}...)::Real\n (such that it can be called as \nf(input...)\n) and return a \nTuple\n where the \ni\nth element is the gradient of \nf\n w.r.t. \ninput[i].\n\n\nNote that \ncfg\n can be preallocated and reused for subsequent calls.\n\n\nIf possible, it is highly recommended to use \nReverseDiff.GradientTape\n to prerecord \nf\n. Otherwise, this method will have to re-record \nf\n's execution trace for every subsequent call.\n\n\nsource\n\n\n#\n\n\nReverseDiff.gradient!\n \n \nFunction\n.\n\n\nReverseDiff.gradient!(result, f, input, cfg::GradientConfig = GradientConfig(input))\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.gradient(f, input, cfg)\n, except it stores the resulting gradient(s) in \nresult\n rather than allocating new memory.\n\n\nresult\n can be an \nAbstractArray\n or a \nTuple\n of \nAbstractArray\ns. The \nresult\n (or any of its elements, if \nisa(result, Tuple)\n), can also be a \nDiffBase.DiffResult\n, in which case the primal value \nf(input)\n (or \nf(input...)\n, if \nisa(input, Tuple)\n) will be stored in it as well.\n\n\nsource\n\n\nReverseDiff.gradient!(tape::Union{GradientTape,CompiledGradient}, input)\n\n\n\n\nIf \ninput\n is an \nAbstractArray\n, assume \ntape\n represents a function of the form \nf(::AbstractArray)::Real\n and return \n\u2207f(input)\n.\n\n\nIf \ninput\n is a tuple of \nAbstractArray\ns, assume \ntape\n represents a function of the form \nf(::AbstractArray...)::Real\n and return a \nTuple\n where the \ni\nth element is the gradient of \nf\n w.r.t. \ninput[i].\n\n\nsource\n\n\nReverseDiff.gradient!(result, tape::Union{GradientTape,CompiledGradient}, input)\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.gradient!(tape, input)\n, except it stores the resulting gradient(s) in \nresult\n rather than allocating new memory.\n\n\nresult\n can be an \nAbstractArray\n or a \nTuple\n of \nAbstractArray\ns. The \nresult\n (or any of its elements, if \nisa(result, Tuple)\n), can also be a \nDiffBase.DiffResult\n, in which case the primal value \nf(input)\n (or \nf(input...)\n, if \nisa(input, Tuple)\n) will be stored in it as well.\n\n\nsource\n\n\n\n\nJacobians of \nf(x::AbstractArray{\n:Real}...)::AbstractArray{\n:Real}\n\n\n#\n\n\nReverseDiff.jacobian\n \n \nFunction\n.\n\n\nReverseDiff.jacobian(f, input, cfg::JacobianConfig = JacobianConfig(input))\n\n\n\n\nIf \ninput\n is an \nAbstractArray\n, assume \nf\n has the form \nf(::AbstractArray{\n:Real})::AbstractArray{\n:Real}\n and return \nJ(f)(input)\n.\n\n\nIf \ninput\n is a tuple of \nAbstractArray\ns, assume \nf\n has the form \nf(::AbstractArray{\n:Real}...)::AbstractArray{\n:Real}\n (such that it can be called as \nf(input...)\n) and return a \nTuple\n where the \ni\nth element is the  Jacobian of \nf\n w.r.t. \ninput[i].\n\n\nNote that \ncfg\n can be preallocated and reused for subsequent calls.\n\n\nIf possible, it is highly recommended to use \nReverseDiff.JacobianTape\n to prerecord \nf\n. Otherwise, this method will have to re-record \nf\n's execution trace for every subsequent call.\n\n\nsource\n\n\nReverseDiff.jacobian(f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))\n\n\n\n\nExactly like \nReverseDiff.jacobian(f, input, cfg)\n, except the target function has the form \nf!(output::AbstractArray{\n:Real}, input::AbstractArray{\n:Real}...)\n.\n\n\nsource\n\n\n#\n\n\nReverseDiff.jacobian!\n \n \nFunction\n.\n\n\nReverseDiff.jacobian!(result, f, input, cfg::JacobianConfig = JacobianConfig(input))\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.jacobian(f, input, cfg)\n, except it stores the resulting Jacobian(s) in \nresult\n rather than allocating new memory.\n\n\nresult\n can be an \nAbstractArray\n or a \nTuple\n of \nAbstractArray\ns. The \nresult\n (or any of its elements, if \nisa(result, Tuple)\n), can also be a \nDiffBase.DiffResult\n, in which case the primal value \nf(input)\n (or \nf(input...)\n, if \nisa(input, Tuple)\n) will be stored in it as well.\n\n\nsource\n\n\nReverseDiff.jacobian!(result, f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))\n\n\n\n\nExactly like \nReverseDiff.jacobian!(result, f, input, cfg)\n, except the target function has the form \nf!(output::AbstractArray{\n:Real}, input::AbstractArray{\n:Real}...)\n.\n\n\nsource\n\n\nReverseDiff.jacobian!(tape::Union{JacobianTape,CompiledJacobian}, input)\n\n\n\n\nIf \ninput\n is an \nAbstractArray\n, assume \ntape\n represents a function of the form \nf(::AbstractArray{\n:Real})::AbstractArray{\n:Real}\n or \nf!(::AbstractArray{\n:Real}, ::AbstractArray{\n:Real})\n and return \ntape\n's Jacobian w.r.t. \ninput\n.\n\n\nIf \ninput\n is a tuple of \nAbstractArray\ns, assume \ntape\n represents a function of the form \nf(::AbstractArray{\n:Real}...)::AbstractArray{\n:Real}\n or \nf!(::AbstractArray{\n:Real}, ::AbstractArray{\n:Real}...)\n and return a \nTuple\n where the \ni\nth element is \ntape\n's Jacobian w.r.t. \ninput[i].\n\n\nNote that if \ntape\n represents a function of the form \nf!(output, input...)\n, you can only execute \ntape\n with new \ninput\n values. There is no way to re-run \ntape\n's tape with new \noutput\n values; since \nf!\n can mutate \noutput\n, there exists no stable \"hook\" for loading new \noutput\n values into the tape.\n\n\nsource\n\n\nReverseDiff.jacobian!(result, tape::Union{JacobianTape,CompiledJacobian}, input)\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.jacobian!(tape, input)\n, except it stores the resulting Jacobian(s) in \nresult\n rather than allocating new memory.\n\n\nresult\n can be an \nAbstractArray\n or a \nTuple\n of \nAbstractArray\ns. The \nresult\n (or any of its elements, if \nisa(result, Tuple)\n), can also be a \nDiffBase.DiffResult\n, in which case the primal value of the target function will be stored in it as well.\n\n\nsource\n\n\n\n\nHessians of \nf(x::AbstractArray{\n:Real})::Real\n\n\n#\n\n\nReverseDiff.hessian\n \n \nFunction\n.\n\n\nReverseDiff.hessian(f, input::AbstractArray, cfg::HessianConfig = HessianConfig(input))\n\n\n\n\nGiven \nf(input::AbstractArray{\n:Real})::Real\n, return \nf\ns Hessian w.r.t. to the given \ninput\n.\n\n\nNote that \ncfg\n can be preallocated and reused for subsequent calls.\n\n\nIf possible, it is highly recommended to use \nReverseDiff.HessianTape\n to prerecord \nf\n. Otherwise, this method will have to re-record \nf\n's execution trace for every subsequent call.\n\n\nsource\n\n\n#\n\n\nReverseDiff.hessian!\n \n \nFunction\n.\n\n\nReverseDiff.hessian!(result::AbstractArray, f, input::AbstractArray, cfg::HessianConfig = HessianConfig(input))\n\nReverseDiff.hessian!(result::DiffResult, f, input::AbstractArray, cfg::HessianConfig = HessianConfig(result, input))\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.hessian(f, input, cfg)\n, except it stores the resulting Hessian in \nresult\n rather than allocating new memory.\n\n\nIf \nresult\n is a \nDiffBase.DiffResult\n, the primal value \nf(input)\n and the gradient \n\u2207f(input)\n will be stored in it along with the Hessian \nH(f)(input)\n.\n\n\nsource\n\n\nReverseDiff.hessian!(tape::Union{HessianTape,CompiledHessian}, input)\n\n\n\n\nAssuming \ntape\n represents a function of the form \nf(::AbstractArray{\n:Real})::Real\n, return the Hessian \nH(f)(input)\n.\n\n\nsource\n\n\nReverseDiff.hessian!(result::AbstractArray, tape::Union{HessianTape,CompiledHessian}, input)\n\nReverseDiff.hessian!(result::DiffResult, tape::Union{HessianTape,CompiledHessian}, input)\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.hessian!(tape, input)\n, except it stores the resulting Hessian in \nresult\n rather than allocating new memory.\n\n\nIf \nresult\n is a \nDiffBase.DiffResult\n, the primal value \nf(input)\n and the gradient \n\u2207f(input)\n will be stored in it along with the Hessian \nH(f)(input)\n.\n\n\nsource\n\n\n\n\nThe \nAbstractTape\n API\n\n\nReverseDiff works by recording the target function's execution trace to a \"tape\", then running the tape forwards and backwards to propagate new input values and derivative information.\n\n\nIn many cases, it is the recording phase of this process that consumes the most time and memory, while the forward and reverse execution passes are often fast and non-allocating. Luckily, ReverseDiff provides the \nAbstractTape\n family of types, which enable the user to \npre-record\n a reusable tape for a given function and differentiation operation.\n\n\nNote that pre-recording a tape can only capture the the execution trace of the target function with the given input values.\n Therefore, re-running the tape (even with new input values) will only execute the paths that were recorded using the original input values. In other words, the tape cannot any re-enact branching behavior that depends on the input values. You can guarantee your own safety in this regard by never using the \nAbstractTape\n API with functions that contain control flow based on the input values.\n\n\nSimilarly to the branching issue, a tape is not guaranteed to capture any side-effects caused or depended on by the target function.\n\n\n#\n\n\nReverseDiff.GradientTape\n \n \nType\n.\n\n\nReverseDiff.GradientTape(f, input, cfg::GradientConfig = GradientConfig(input))\n\n\n\n\nReturn a \nGradientTape\n instance containing a pre-recorded execution trace of \nf\n at the given \ninput\n.\n\n\nThis \nGradientTape\n can then be passed to \nReverseDiff.gradient!\n to take gradients of the execution trace with new \ninput\n values. Note that these new values must have the same element type and shape as \ninput\n.\n\n\nSee \nReverseDiff.gradient\n for a description of acceptable types for \ninput\n.\n\n\nsource\n\n\n#\n\n\nReverseDiff.JacobianTape\n \n \nType\n.\n\n\nReverseDiff.JacobianTape(f, input, cfg::JacobianConfig = JacobianConfig(input))\n\n\n\n\nReturn a \nJacobianTape\n instance containing a pre-recorded execution trace of \nf\n at the given \ninput\n.\n\n\nThis \nJacobianTape\n can then be passed to \nReverseDiff.jacobian!\n to take Jacobians of the execution trace with new \ninput\n values. Note that these new values must have the same element type and shape as \ninput\n.\n\n\nSee \nReverseDiff.jacobian\n for a description of acceptable types for \ninput\n.\n\n\nsource\n\n\nReverseDiff.JacobianTape(f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))\n\n\n\n\nReturn a \nJacobianTape\n instance containing a pre-recorded execution trace of \nf\n at the given \noutput\n and \ninput\n.\n\n\nThis \nJacobianTape\n can then be passed to \nReverseDiff.jacobian!\n to take Jacobians of the execution trace with new \ninput\n values. Note that these new values must have the same element type and shape as \ninput\n.\n\n\nSee \nReverseDiff.jacobian\n for a description of acceptable types for \ninput\n.\n\n\nsource\n\n\n#\n\n\nReverseDiff.HessianTape\n \n \nType\n.\n\n\nReverseDiff.HessianTape(f, input, cfg::HessianConfig = HessianConfig(input))\n\n\n\n\nReturn a \nHessianTape\n instance containing a pre-recorded execution trace of \nf\n at the given \ninput\n.\n\n\nThis \nHessianTape\n can then be passed to \nReverseDiff.hessian!\n to take Hessians of the execution trace with new \ninput\n values. Note that these new values must have the same element type and shape as \ninput\n.\n\n\nSee \nReverseDiff.hessian\n for a description of acceptable types for \ninput\n.\n\n\nsource\n\n\n#\n\n\nReverseDiff.compile\n \n \nFunction\n.\n\n\nReverseDiff.compile(t::AbstractTape)\n\n\n\n\nReturn a fully compiled representation of \nt\n of type \nCompiledTape\n. This object can be passed to any API methods that accept \nt\n (e.g. \ngradient!(result, t, input)\n).\n\n\nIn many cases, compiling \nt\n can significantly speed up execution time. Note that the longer the tape, the more time compilation may take. Very long tapes (i.e. when \nlength(t)\n is on the order of 10000 elements) can take a very long time to compile.\n\n\nNote that this function calls \neval\n in the \ncurrent_module()\n to generate functions from \nt\n. Thus, the returned \nCompiledTape\n will only be useable once the world-age counter has caught up with the world-age of the \neval\n'd functions (i.e. once the call stack has bubbled up to top level).\n\n\nsource\n\n\n\n\nThe \nAbstractConfig\n API\n\n\nFor the sake of convenience and performance, all \"extra\" information used by ReverseDiff's API methods is bundled up in the \nReverseDiff.AbstractConfig\n family of types. These types allow the user to easily feed several different parameters to ReverseDiff's API methods, such as work buffers and tape configurations.\n\n\nReverseDiff's basic API methods will allocate these types automatically by default, but you can reduce memory usage and improve performance if you preallocate them yourself.\n\n\n#\n\n\nReverseDiff.GradientConfig\n \n \nType\n.\n\n\nReverseDiff.GradientConfig(input, tp::RawTape = RawTape())\n\n\n\n\nReturn a \nGradientConfig\n instance containing the preallocated tape and work buffers used by the \nReverseDiff.gradient\n/\nReverseDiff.gradient!\n methods.\n\n\nNote that \ninput\n is only used for type and shape information; it is not stored or modified in any way. It is assumed that the element type of \ninput\n is same as the element type of the target function's output.\n\n\nSee \nReverseDiff.gradient\n for a description of acceptable types for \ninput\n.\n\n\nsource\n\n\nReverseDiff.GradientConfig(input, ::Type{D}, tp::RawTape = RawTape())\n\n\n\n\nLike \nGradientConfig(input, tp)\n, except the provided type \nD\n is assumed to be the element type of the target function's output.\n\n\nsource\n\n\n#\n\n\nReverseDiff.JacobianConfig\n \n \nType\n.\n\n\nReverseDiff.JacobianConfig(input, tp::RawTape = RawTape())\n\n\n\n\nReturn a \nJacobianConfig\n instance containing the preallocated tape and work buffers used by the \nReverseDiff.jacobian\n/\nReverseDiff.jacobian!\n methods.\n\n\nNote that \ninput\n is only used for type and shape information; it is not stored or modified in any way. It is assumed that the element type of \ninput\n is same as the element type of the target function's output.\n\n\nSee \nReverseDiff.jacobian\n for a description of acceptable types for \ninput\n.\n\n\nReverseDiff.JacobianConfig(input, ::Type{D}, tp::RawTape = RawTape())\n\n\n\n\nLike \nJacobianConfig(input, tp)\n, except the provided type \nD\n is assumed to be the element type of the target function's output.\n\n\nsource\n\n\nReverseDiff.JacobianConfig(output::AbstractArray, input, tp::RawTape = RawTape())\n\n\n\n\nReturn a \nJacobianConfig\n instance containing the preallocated tape and work buffers used by the \nReverseDiff.jacobian\n/\nReverseDiff.jacobian!\n methods. This method assumes the target function has the form \nf!(output, input)\n\n\nNote that \ninput\n and \noutput\n are only used for type and shape information; they are not stored or modified in any way.\n\n\nSee \nReverseDiff.jacobian\n for a description of acceptable types for \ninput\n.\n\n\nsource\n\n\nReverseDiff.JacobianConfig(result::DiffBase.DiffResult, input, tp::RawTape = RawTape())\n\n\n\n\nA convenience method for \nJacobianConfig(DiffBase.value(result), input, tp)\n.\n\n\nsource\n\n\n#\n\n\nReverseDiff.HessianConfig\n \n \nType\n.\n\n\nReverseDiff.HessianConfig(input::AbstractArray, gtp::RawTape = RawTape(), jtp::RawTape = RawTape())\n\n\n\n\nReturn a \nHessianConfig\n instance containing the preallocated tape and work buffers used by the \nReverseDiff.hessian\n/\nReverseDiff.hessian!\n methods. \ngtp\n is the tape used for the inner gradient calculation, while \njtp\n is used for outer Jacobian calculation.\n\n\nNote that \ninput\n is only used for type and shape information; it is not stored or modified in any way. It is assumed that the element type of \ninput\n is same as the element type of the target function's output.\n\n\nsource\n\n\nReverseDiff.HessianConfig(input::AbstractArray, ::Type{D}, gtp::RawTape = RawTape(), jtp::RawTape = RawTape())\n\n\n\n\nLike \nHessianConfig(input, tp)\n, except the provided type \nD\n is assumed to be the element type of the target function's output.\n\n\nsource\n\n\nReverseDiff.HessianConfig(result::DiffBase.DiffResult, input::AbstractArray, gtp::RawTape = RawTape(), jtp::RawTape = RawTape())\n\n\n\n\nLike \nHessianConfig(input, tp)\n, but utilize \nresult\n along with \ninput\n to construct work buffers.\n\n\nNote that \nresult\n and \ninput\n are only used for type and shape information; they are not stored or modified in any way.\n\n\nsource\n\n\n\n\nOptimization Annotations\n\n\n#\n\n\nReverseDiff.@forward\n \n \nMacro\n.\n\n\nReverseDiff.@forward(f)(args::Real...)\nReverseDiff.@forward f(args::Real...) = ...\nReverseDiff.@forward f = (args::Real...) -\n ...\n\n\n\n\nDeclare that the given function should be differentiated using forward mode automatic differentiation. Note that the macro can be used at either the definition site or at the call site of \nf\n. Currently, only \nlength(args) \n= 2\n is supported. \nNote that, if \nf\n is defined within another function \ng\n, \nf\n should not close over any differentiable input of \ng\n.\n By using this macro, you are providing a guarantee that this property holds true.\n\n\nThis macro can be very beneficial for performance when intermediate functions in your computation are low dimensional scalar functions, because it minimizes the number of instructions that must be recorded to the tape. For example, take the function \nsigmoid(n) = 1. / (1. + exp(-n))\n. Normally, using ReverseDiff to differentiate this function would require recording 4 instructions (\n-\n, \nexp\n, \n+\n, and \n/\n). However, if we apply the \n@forward\n macro, only one instruction will be recorded (\nsigmoid\n). The \nsigmoid\n function will then be differentiated using ForwardDiff's \nDual\n number type.\n\n\nThis is also beneficial for higher-order elementwise function application. ReverseDiff overloads \nmap\n/\nbroadcast\n to dispatch on \n@forward\n-applied functions. For example, \nmap(@forward(f), x)\n will usually be more performant than \nmap(f, x)\n.\n\n\nReverseDiff overloads many Base scalar functions to behave as \n@forward\n functions by default. A full list is given by \nReverseDiff.FORWARD_UNARY_SCALAR_FUNCS\n and \nReverseDiff.FORWARD_BINARY_SCALAR_FUNCS\n.\n\n\nsource\n\n\n#\n\n\nReverseDiff.@skip\n \n \nMacro\n.\n\n\nReverseDiff.@skip(f)(args::Real...)\nReverseDiff.@skip f(args::Real...) = ...\nReverseDiff.@skip f = (args::Real...) -\n ...\n\n\n\n\nDeclare that the given function should be skipped during the instruction-recording phase of differentiation. Note that the macro can be used at either the definition site or at the call site of \nf\n. \nNote that, if \nf\n is defined within another function \ng\n, \nf\n should not close over any differentiable input of \ng\n.\n By using this macro, you are providing a guarantee that this property holds true.\n\n\nReverseDiff overloads many Base scalar functions to behave as \n@skip\n functions by default. A full list is given by \nReverseDiff.SKIPPED_UNARY_SCALAR_FUNCS\n and \nReverseDiff.SKIPPED_BINARY_SCALAR_FUNCS\n.\n\n\nsource", 
            "title": "ReverseDiff API"
        }, 
        {
            "location": "/api/#reversediff-api", 
            "text": "", 
            "title": "ReverseDiff API"
        }, 
        {
            "location": "/api/#gradients-of-fxabstractarrayrealreal", 
            "text": "#  ReverseDiff.gradient     Function .  ReverseDiff.gradient(f, input, cfg::GradientConfig = GradientConfig(input))  If  input  is an  AbstractArray , assume  f  has the form  f(::AbstractArray{ :Real})::Real  and return  \u2207f(input) .  If  input  is a tuple of  AbstractArray s, assume  f  has the form  f(::AbstractArray{ :Real}...)::Real  (such that it can be called as  f(input...) ) and return a  Tuple  where the  i th element is the gradient of  f  w.r.t.  input[i].  Note that  cfg  can be preallocated and reused for subsequent calls.  If possible, it is highly recommended to use  ReverseDiff.GradientTape  to prerecord  f . Otherwise, this method will have to re-record  f 's execution trace for every subsequent call.  source  #  ReverseDiff.gradient!     Function .  ReverseDiff.gradient!(result, f, input, cfg::GradientConfig = GradientConfig(input))  Returns  result . This method is exactly like  ReverseDiff.gradient(f, input, cfg) , except it stores the resulting gradient(s) in  result  rather than allocating new memory.  result  can be an  AbstractArray  or a  Tuple  of  AbstractArray s. The  result  (or any of its elements, if  isa(result, Tuple) ), can also be a  DiffBase.DiffResult , in which case the primal value  f(input)  (or  f(input...) , if  isa(input, Tuple) ) will be stored in it as well.  source  ReverseDiff.gradient!(tape::Union{GradientTape,CompiledGradient}, input)  If  input  is an  AbstractArray , assume  tape  represents a function of the form  f(::AbstractArray)::Real  and return  \u2207f(input) .  If  input  is a tuple of  AbstractArray s, assume  tape  represents a function of the form  f(::AbstractArray...)::Real  and return a  Tuple  where the  i th element is the gradient of  f  w.r.t.  input[i].  source  ReverseDiff.gradient!(result, tape::Union{GradientTape,CompiledGradient}, input)  Returns  result . This method is exactly like  ReverseDiff.gradient!(tape, input) , except it stores the resulting gradient(s) in  result  rather than allocating new memory.  result  can be an  AbstractArray  or a  Tuple  of  AbstractArray s. The  result  (or any of its elements, if  isa(result, Tuple) ), can also be a  DiffBase.DiffResult , in which case the primal value  f(input)  (or  f(input...) , if  isa(input, Tuple) ) will be stored in it as well.  source", 
            "title": "Gradients of f(x::AbstractArray{&lt;:Real}...)::Real"
        }, 
        {
            "location": "/api/#jacobians-of-fxabstractarrayrealabstractarrayreal", 
            "text": "#  ReverseDiff.jacobian     Function .  ReverseDiff.jacobian(f, input, cfg::JacobianConfig = JacobianConfig(input))  If  input  is an  AbstractArray , assume  f  has the form  f(::AbstractArray{ :Real})::AbstractArray{ :Real}  and return  J(f)(input) .  If  input  is a tuple of  AbstractArray s, assume  f  has the form  f(::AbstractArray{ :Real}...)::AbstractArray{ :Real}  (such that it can be called as  f(input...) ) and return a  Tuple  where the  i th element is the  Jacobian of  f  w.r.t.  input[i].  Note that  cfg  can be preallocated and reused for subsequent calls.  If possible, it is highly recommended to use  ReverseDiff.JacobianTape  to prerecord  f . Otherwise, this method will have to re-record  f 's execution trace for every subsequent call.  source  ReverseDiff.jacobian(f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))  Exactly like  ReverseDiff.jacobian(f, input, cfg) , except the target function has the form  f!(output::AbstractArray{ :Real}, input::AbstractArray{ :Real}...) .  source  #  ReverseDiff.jacobian!     Function .  ReverseDiff.jacobian!(result, f, input, cfg::JacobianConfig = JacobianConfig(input))  Returns  result . This method is exactly like  ReverseDiff.jacobian(f, input, cfg) , except it stores the resulting Jacobian(s) in  result  rather than allocating new memory.  result  can be an  AbstractArray  or a  Tuple  of  AbstractArray s. The  result  (or any of its elements, if  isa(result, Tuple) ), can also be a  DiffBase.DiffResult , in which case the primal value  f(input)  (or  f(input...) , if  isa(input, Tuple) ) will be stored in it as well.  source  ReverseDiff.jacobian!(result, f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))  Exactly like  ReverseDiff.jacobian!(result, f, input, cfg) , except the target function has the form  f!(output::AbstractArray{ :Real}, input::AbstractArray{ :Real}...) .  source  ReverseDiff.jacobian!(tape::Union{JacobianTape,CompiledJacobian}, input)  If  input  is an  AbstractArray , assume  tape  represents a function of the form  f(::AbstractArray{ :Real})::AbstractArray{ :Real}  or  f!(::AbstractArray{ :Real}, ::AbstractArray{ :Real})  and return  tape 's Jacobian w.r.t.  input .  If  input  is a tuple of  AbstractArray s, assume  tape  represents a function of the form  f(::AbstractArray{ :Real}...)::AbstractArray{ :Real}  or  f!(::AbstractArray{ :Real}, ::AbstractArray{ :Real}...)  and return a  Tuple  where the  i th element is  tape 's Jacobian w.r.t.  input[i].  Note that if  tape  represents a function of the form  f!(output, input...) , you can only execute  tape  with new  input  values. There is no way to re-run  tape 's tape with new  output  values; since  f!  can mutate  output , there exists no stable \"hook\" for loading new  output  values into the tape.  source  ReverseDiff.jacobian!(result, tape::Union{JacobianTape,CompiledJacobian}, input)  Returns  result . This method is exactly like  ReverseDiff.jacobian!(tape, input) , except it stores the resulting Jacobian(s) in  result  rather than allocating new memory.  result  can be an  AbstractArray  or a  Tuple  of  AbstractArray s. The  result  (or any of its elements, if  isa(result, Tuple) ), can also be a  DiffBase.DiffResult , in which case the primal value of the target function will be stored in it as well.  source", 
            "title": "Jacobians of f(x::AbstractArray{&lt;:Real}...)::AbstractArray{&lt;:Real}"
        }, 
        {
            "location": "/api/#hessians-of-fxabstractarrayrealreal", 
            "text": "#  ReverseDiff.hessian     Function .  ReverseDiff.hessian(f, input::AbstractArray, cfg::HessianConfig = HessianConfig(input))  Given  f(input::AbstractArray{ :Real})::Real , return  f s Hessian w.r.t. to the given  input .  Note that  cfg  can be preallocated and reused for subsequent calls.  If possible, it is highly recommended to use  ReverseDiff.HessianTape  to prerecord  f . Otherwise, this method will have to re-record  f 's execution trace for every subsequent call.  source  #  ReverseDiff.hessian!     Function .  ReverseDiff.hessian!(result::AbstractArray, f, input::AbstractArray, cfg::HessianConfig = HessianConfig(input))\n\nReverseDiff.hessian!(result::DiffResult, f, input::AbstractArray, cfg::HessianConfig = HessianConfig(result, input))  Returns  result . This method is exactly like  ReverseDiff.hessian(f, input, cfg) , except it stores the resulting Hessian in  result  rather than allocating new memory.  If  result  is a  DiffBase.DiffResult , the primal value  f(input)  and the gradient  \u2207f(input)  will be stored in it along with the Hessian  H(f)(input) .  source  ReverseDiff.hessian!(tape::Union{HessianTape,CompiledHessian}, input)  Assuming  tape  represents a function of the form  f(::AbstractArray{ :Real})::Real , return the Hessian  H(f)(input) .  source  ReverseDiff.hessian!(result::AbstractArray, tape::Union{HessianTape,CompiledHessian}, input)\n\nReverseDiff.hessian!(result::DiffResult, tape::Union{HessianTape,CompiledHessian}, input)  Returns  result . This method is exactly like  ReverseDiff.hessian!(tape, input) , except it stores the resulting Hessian in  result  rather than allocating new memory.  If  result  is a  DiffBase.DiffResult , the primal value  f(input)  and the gradient  \u2207f(input)  will be stored in it along with the Hessian  H(f)(input) .  source", 
            "title": "Hessians of f(x::AbstractArray{&lt;:Real})::Real"
        }, 
        {
            "location": "/api/#the-abstracttape-api", 
            "text": "ReverseDiff works by recording the target function's execution trace to a \"tape\", then running the tape forwards and backwards to propagate new input values and derivative information.  In many cases, it is the recording phase of this process that consumes the most time and memory, while the forward and reverse execution passes are often fast and non-allocating. Luckily, ReverseDiff provides the  AbstractTape  family of types, which enable the user to  pre-record  a reusable tape for a given function and differentiation operation.  Note that pre-recording a tape can only capture the the execution trace of the target function with the given input values.  Therefore, re-running the tape (even with new input values) will only execute the paths that were recorded using the original input values. In other words, the tape cannot any re-enact branching behavior that depends on the input values. You can guarantee your own safety in this regard by never using the  AbstractTape  API with functions that contain control flow based on the input values.  Similarly to the branching issue, a tape is not guaranteed to capture any side-effects caused or depended on by the target function.  #  ReverseDiff.GradientTape     Type .  ReverseDiff.GradientTape(f, input, cfg::GradientConfig = GradientConfig(input))  Return a  GradientTape  instance containing a pre-recorded execution trace of  f  at the given  input .  This  GradientTape  can then be passed to  ReverseDiff.gradient!  to take gradients of the execution trace with new  input  values. Note that these new values must have the same element type and shape as  input .  See  ReverseDiff.gradient  for a description of acceptable types for  input .  source  #  ReverseDiff.JacobianTape     Type .  ReverseDiff.JacobianTape(f, input, cfg::JacobianConfig = JacobianConfig(input))  Return a  JacobianTape  instance containing a pre-recorded execution trace of  f  at the given  input .  This  JacobianTape  can then be passed to  ReverseDiff.jacobian!  to take Jacobians of the execution trace with new  input  values. Note that these new values must have the same element type and shape as  input .  See  ReverseDiff.jacobian  for a description of acceptable types for  input .  source  ReverseDiff.JacobianTape(f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))  Return a  JacobianTape  instance containing a pre-recorded execution trace of  f  at the given  output  and  input .  This  JacobianTape  can then be passed to  ReverseDiff.jacobian!  to take Jacobians of the execution trace with new  input  values. Note that these new values must have the same element type and shape as  input .  See  ReverseDiff.jacobian  for a description of acceptable types for  input .  source  #  ReverseDiff.HessianTape     Type .  ReverseDiff.HessianTape(f, input, cfg::HessianConfig = HessianConfig(input))  Return a  HessianTape  instance containing a pre-recorded execution trace of  f  at the given  input .  This  HessianTape  can then be passed to  ReverseDiff.hessian!  to take Hessians of the execution trace with new  input  values. Note that these new values must have the same element type and shape as  input .  See  ReverseDiff.hessian  for a description of acceptable types for  input .  source  #  ReverseDiff.compile     Function .  ReverseDiff.compile(t::AbstractTape)  Return a fully compiled representation of  t  of type  CompiledTape . This object can be passed to any API methods that accept  t  (e.g.  gradient!(result, t, input) ).  In many cases, compiling  t  can significantly speed up execution time. Note that the longer the tape, the more time compilation may take. Very long tapes (i.e. when  length(t)  is on the order of 10000 elements) can take a very long time to compile.  Note that this function calls  eval  in the  current_module()  to generate functions from  t . Thus, the returned  CompiledTape  will only be useable once the world-age counter has caught up with the world-age of the  eval 'd functions (i.e. once the call stack has bubbled up to top level).  source", 
            "title": "The AbstractTape API"
        }, 
        {
            "location": "/api/#the-abstractconfig-api", 
            "text": "For the sake of convenience and performance, all \"extra\" information used by ReverseDiff's API methods is bundled up in the  ReverseDiff.AbstractConfig  family of types. These types allow the user to easily feed several different parameters to ReverseDiff's API methods, such as work buffers and tape configurations.  ReverseDiff's basic API methods will allocate these types automatically by default, but you can reduce memory usage and improve performance if you preallocate them yourself.  #  ReverseDiff.GradientConfig     Type .  ReverseDiff.GradientConfig(input, tp::RawTape = RawTape())  Return a  GradientConfig  instance containing the preallocated tape and work buffers used by the  ReverseDiff.gradient / ReverseDiff.gradient!  methods.  Note that  input  is only used for type and shape information; it is not stored or modified in any way. It is assumed that the element type of  input  is same as the element type of the target function's output.  See  ReverseDiff.gradient  for a description of acceptable types for  input .  source  ReverseDiff.GradientConfig(input, ::Type{D}, tp::RawTape = RawTape())  Like  GradientConfig(input, tp) , except the provided type  D  is assumed to be the element type of the target function's output.  source  #  ReverseDiff.JacobianConfig     Type .  ReverseDiff.JacobianConfig(input, tp::RawTape = RawTape())  Return a  JacobianConfig  instance containing the preallocated tape and work buffers used by the  ReverseDiff.jacobian / ReverseDiff.jacobian!  methods.  Note that  input  is only used for type and shape information; it is not stored or modified in any way. It is assumed that the element type of  input  is same as the element type of the target function's output.  See  ReverseDiff.jacobian  for a description of acceptable types for  input .  ReverseDiff.JacobianConfig(input, ::Type{D}, tp::RawTape = RawTape())  Like  JacobianConfig(input, tp) , except the provided type  D  is assumed to be the element type of the target function's output.  source  ReverseDiff.JacobianConfig(output::AbstractArray, input, tp::RawTape = RawTape())  Return a  JacobianConfig  instance containing the preallocated tape and work buffers used by the  ReverseDiff.jacobian / ReverseDiff.jacobian!  methods. This method assumes the target function has the form  f!(output, input)  Note that  input  and  output  are only used for type and shape information; they are not stored or modified in any way.  See  ReverseDiff.jacobian  for a description of acceptable types for  input .  source  ReverseDiff.JacobianConfig(result::DiffBase.DiffResult, input, tp::RawTape = RawTape())  A convenience method for  JacobianConfig(DiffBase.value(result), input, tp) .  source  #  ReverseDiff.HessianConfig     Type .  ReverseDiff.HessianConfig(input::AbstractArray, gtp::RawTape = RawTape(), jtp::RawTape = RawTape())  Return a  HessianConfig  instance containing the preallocated tape and work buffers used by the  ReverseDiff.hessian / ReverseDiff.hessian!  methods.  gtp  is the tape used for the inner gradient calculation, while  jtp  is used for outer Jacobian calculation.  Note that  input  is only used for type and shape information; it is not stored or modified in any way. It is assumed that the element type of  input  is same as the element type of the target function's output.  source  ReverseDiff.HessianConfig(input::AbstractArray, ::Type{D}, gtp::RawTape = RawTape(), jtp::RawTape = RawTape())  Like  HessianConfig(input, tp) , except the provided type  D  is assumed to be the element type of the target function's output.  source  ReverseDiff.HessianConfig(result::DiffBase.DiffResult, input::AbstractArray, gtp::RawTape = RawTape(), jtp::RawTape = RawTape())  Like  HessianConfig(input, tp) , but utilize  result  along with  input  to construct work buffers.  Note that  result  and  input  are only used for type and shape information; they are not stored or modified in any way.  source", 
            "title": "The AbstractConfig API"
        }, 
        {
            "location": "/api/#optimization-annotations", 
            "text": "#  ReverseDiff.@forward     Macro .  ReverseDiff.@forward(f)(args::Real...)\nReverseDiff.@forward f(args::Real...) = ...\nReverseDiff.@forward f = (args::Real...) -  ...  Declare that the given function should be differentiated using forward mode automatic differentiation. Note that the macro can be used at either the definition site or at the call site of  f . Currently, only  length(args)  = 2  is supported.  Note that, if  f  is defined within another function  g ,  f  should not close over any differentiable input of  g .  By using this macro, you are providing a guarantee that this property holds true.  This macro can be very beneficial for performance when intermediate functions in your computation are low dimensional scalar functions, because it minimizes the number of instructions that must be recorded to the tape. For example, take the function  sigmoid(n) = 1. / (1. + exp(-n)) . Normally, using ReverseDiff to differentiate this function would require recording 4 instructions ( - ,  exp ,  + , and  / ). However, if we apply the  @forward  macro, only one instruction will be recorded ( sigmoid ). The  sigmoid  function will then be differentiated using ForwardDiff's  Dual  number type.  This is also beneficial for higher-order elementwise function application. ReverseDiff overloads  map / broadcast  to dispatch on  @forward -applied functions. For example,  map(@forward(f), x)  will usually be more performant than  map(f, x) .  ReverseDiff overloads many Base scalar functions to behave as  @forward  functions by default. A full list is given by  ReverseDiff.FORWARD_UNARY_SCALAR_FUNCS  and  ReverseDiff.FORWARD_BINARY_SCALAR_FUNCS .  source  #  ReverseDiff.@skip     Macro .  ReverseDiff.@skip(f)(args::Real...)\nReverseDiff.@skip f(args::Real...) = ...\nReverseDiff.@skip f = (args::Real...) -  ...  Declare that the given function should be skipped during the instruction-recording phase of differentiation. Note that the macro can be used at either the definition site or at the call site of  f .  Note that, if  f  is defined within another function  g ,  f  should not close over any differentiable input of  g .  By using this macro, you are providing a guarantee that this property holds true.  ReverseDiff overloads many Base scalar functions to behave as  @skip  functions by default. A full list is given by  ReverseDiff.SKIPPED_UNARY_SCALAR_FUNCS  and  ReverseDiff.SKIPPED_BINARY_SCALAR_FUNCS .  source", 
            "title": "Optimization Annotations"
        }
    ]
}